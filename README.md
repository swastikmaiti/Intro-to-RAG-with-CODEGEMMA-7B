# RAG with local LLM

llamafile@https://github.com/Mozilla-Ocho/llamafile.git

llamafile enable execution and distribution of LLM with a single file.
All we need is to download LLM weights in GGUF format and llamfile.exe.
We can run the file across varous OS and platforms.

On execution llamafile start a local server for interacting with LLM via UI.
Developer can choose to interact with Curl API via CLI or with OpenAI API via python.


LLM used @[TheBloke/Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)
